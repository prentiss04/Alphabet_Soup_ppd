For the initial trial of the deep neural net, I chose two layers with 8 and 5 nodes respectively with 100 epochs. This wasn’t driven by some particular insight but rather just as a starting point similar to the ending point of the course material. With the volume of inputs (48), the fitting took several minutes and an accuracy of 0.7271 and loss of 0.5522. 

For the second trial, I increased the nodes in each layer to 150 each using a 3x input quantity to see if I could both improve the accuracy and decrease the loss. For the first trial, I found that accuracy began to reach diminishing returns around 20 epochs. In light of the larger volume of nodes and the potential to shorten the model fitting, I reduced the second trial to 20 epochs, ceteris paribus. The second trial achieved accuracy of 0.7294 and loss of 0.5824. Not exactly the gain I was hoping for. Though not provided in the output increasing to 100 epochs did little to improve performance, as expected. 

With the goal accuracy of 0.75 within reach, I would aim to decrease the learning loss by trying a tanh activation. It’s possible that a different model may be more appropriate for a dataset that features nearly 50 features.